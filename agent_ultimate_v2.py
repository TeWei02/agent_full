import os
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.llms.ollama import Ollama
from llama_index.llms.groq import Groq
from llama_index.embeddings.ollama import OllamaEmbedding

print("ğŸš€ ModernReader çµ‚æ¥µå…è²» AI Agent å•Ÿå‹•ä¸­...\n")

# ============ é…ç½®é¸é … ============
USE_GROQ = True  # True = ä½¿ç”¨ Groqï¼ˆå¿«ï¼‰ï¼ŒFalse = ä½¿ç”¨ Ollamaï¼ˆæœ¬åœ°ï¼‰
GROQ_API_KEY = os.getenv("GROQ_API_KEY", "")  # å¡«å…¥ä½ çš„ Groq Key

# ============ è¨­å®š LLM ============
if USE_GROQ:
    print("ğŸŒ ä½¿ç”¨ Groq é›²ç«¯ APIï¼ˆé€Ÿåº¦å¿«ï¼‰...")
    os.environ["GROQ_API_KEY"] = GROQ_API_KEY
    llm = Groq(
        model="llama-3.3-70b-versatile",
        api_key=GROQ_API_KEY
    )
    Settings.llm = llm
    print("âœ… Groq LLM å·²é€£æ¥\n")
else:
    print("ğŸ–¥ï¸  ä½¿ç”¨æœ¬åœ° Ollamaï¼ˆå®Œå…¨å…è²»ï¼‰...")
    llm = Ollama(
        model="llama3:latest",  # ä½¿ç”¨ä½ å·²æœ‰çš„æ¨¡å‹
        request_timeout=120.0
    )
    Settings.llm = llm
    print("âœ… Ollama å·²é€£æ¥\n")

# ============ è¨­å®š Embeddingï¼ˆç”¨ Ollamaï¼Œä¸éœ€è¦é¡å¤–å¥—ä»¶ï¼‰============
print("ğŸ“Š è¼‰å…¥ Ollama Embedding æ¨¡å‹...")
Settings.embed_model = OllamaEmbedding(
    model_name="llama3:latest",  # ä½¿ç”¨ä½ å·²æœ‰çš„æ¨¡å‹åš embedding
    base_url="http://localhost:11434"
)
print("âœ… Embedding å·²æº–å‚™\n")

# ============ è¼‰å…¥ PDF ============
print("ğŸ“„ è¼‰å…¥ PDF æ–‡ä»¶...")
try:
    documents = SimpleDirectoryReader(
        input_files=["sample.pdf"]
    ).load_data()
    print(f"âœ… æˆåŠŸè¼‰å…¥ {len(documents)} å€‹æ–‡ä»¶")
    print(f"   ç¸½å­—æ•¸: {sum(len(doc.text) for doc in documents)} å­—\n")
except Exception as e:
    print(f"âŒ è¼‰å…¥å¤±æ•—: {e}\n")
    exit(1)

# ============ å»ºç«‹å‘é‡ç´¢å¼• ============
print("ğŸ” å»ºç«‹å‘é‡ç´¢å¼•ï¼ˆé€™å¯èƒ½éœ€è¦ä¸€é»æ™‚é–“ï¼‰...")
try:
    index = VectorStoreIndex.from_documents(documents, show_progress=True)
    print("âœ… ç´¢å¼•å»ºç«‹å®Œæˆ\n")
except Exception as e:
    print(f"âŒ ç´¢å¼•å»ºç«‹å¤±æ•—: {e}\n")
    exit(1)

# ============ å»ºç«‹æŸ¥è©¢å¼•æ“ ============
print("âš™ï¸  è¨­ç½®æŸ¥è©¢å¼•æ“...")
query_engine = index.as_query_engine(
    similarity_top_k=3,
    streaming=False
)
print("âœ… æŸ¥è©¢å¼•æ“å·²æº–å‚™\n")

# ============ è‡ªå‹•æ¸¬è©¦ ============
print("="*60)
print("ğŸ¯ AI Agent å·²æº–å‚™å¥½ï¼é–‹å§‹è‡ªå‹•æ¸¬è©¦")
print("="*60 + "\n")

test_queries = [
    "é€™å€‹æ–‡ä»¶ä¸»è¦è¬›ä»€éº¼ï¼Ÿè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”",
    "æ–‡ä»¶ä¸­æåˆ°å“ªäº›æŠ€è¡“ï¼Ÿ",
]

for i, query in enumerate(test_queries, 1):
    print(f"ğŸ“ æ¸¬è©¦ {i}/{len(test_queries)}: {query}")
    try:
        response = query_engine.query(query)
        print(f"ğŸ¤– å›ç­”: {response}\n")
    except Exception as e:
        print(f"âŒ æŸ¥è©¢å¤±æ•—: {e}\n")

# ============ äº’å‹•æ¨¡å¼ ============
print("="*60)
print("ğŸ’¬ é€²å…¥äº’å‹•æ¨¡å¼")
print("="*60)
print("æŒ‡ä»¤:")
print("  - è¼¸å…¥å•é¡Œï¼šç›´æ¥æå•")
print("  - 'models': é¡¯ç¤ºå¯ç”¨çš„ Ollama æ¨¡å‹")
print("  - 'use <model>': åˆ‡æ›æ¨¡å‹ (ä¾‹å¦‚: use mistral:latest)")
print("  - 'switch': åˆ‡æ› Groq/Ollama")
print("  - 'info': é¡¯ç¤ºç•¶å‰é…ç½®")
print("  - 'exit': é€€å‡º\n")

current_model = "llama3:latest"

while True:
    try:
        user_input = input("ä½ : ").strip()
        
        if not user_input:
            continue
            
        if user_input.lower() in ['exit', 'quit', 'q']:
            print("\nğŸ‘‹ æ„Ÿè¬ä½¿ç”¨ ModernReader AI Agentï¼å†è¦‹~")
            break
        
        if user_input.lower() == 'models':
            print("\nğŸ“¦ ä½ å·²ä¸‹è¼‰çš„æ¨¡å‹:")
            print("  - llama3:latest (4.7 GB)")
            print("  - llama2:latest (3.8 GB)")
            print("  - mistral:latest (4.4 GB)")
            print("  - phi:latest (1.6 GB)")
            print("  - phi3:latest (2.2 GB)")
            print(f"\nç•¶å‰ä½¿ç”¨: {current_model}\n")
            continue
        
        if user_input.lower().startswith('use '):
            new_model = user_input[4:].strip()
            try:
                Settings.llm = Ollama(model=new_model, request_timeout=120.0)
                Settings.embed_model = OllamaEmbedding(model_name=new_model)
                query_engine = index.as_query_engine(similarity_top_k=3)
                current_model = new_model
                print(f"âœ… å·²åˆ‡æ›åˆ° {new_model}\n")
            except Exception as e:
                print(f"âŒ åˆ‡æ›å¤±æ•—: {e}\n")
            continue
        
        if user_input.lower() == 'switch':
            USE_GROQ = not USE_GROQ
            if USE_GROQ:
                Settings.llm = Groq(model="llama-3.1-70b-versatile", api_key=GROQ_API_KEY)
                print("âœ… å·²åˆ‡æ›åˆ° Groqï¼ˆé›²ç«¯ï¼‰\n")
            else:
                Settings.llm = Ollama(model=current_model, request_timeout=120.0)
                print(f"âœ… å·²åˆ‡æ›åˆ° Ollama (æœ¬åœ° - {current_model})\n")
            query_engine = index.as_query_engine(similarity_top_k=3)
            continue
        
        if user_input.lower() == 'info':
            llm_type = "Groq (é›²ç«¯)" if USE_GROQ else f"Ollama (æœ¬åœ° - {current_model})"
            print(f"\nğŸ“Š ç•¶å‰é…ç½®:")
            print(f"   LLM: {llm_type}")
            print(f"   Embedding: Ollama ({current_model})")
            print(f"   æ–‡ä»¶æ•¸: {len(documents)}")
            print(f"   ç¸½å­—æ•¸: {sum(len(doc.text) for doc in documents)}\n")
            continue
        
        # æ­£å¸¸æŸ¥è©¢
        print("ğŸ¤– æ€è€ƒä¸­...")
        response = query_engine.query(user_input)
        print(f"\n{response}\n")
        
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ å·²ä¸­æ–·ï¼Œå†è¦‹ï¼")
        break
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}\n")
        print("æç¤º: å¦‚æœæ˜¯ Groq API éŒ¯èª¤ï¼Œè©¦è©¦è¼¸å…¥ 'switch' åˆ‡æ›åˆ° Ollama\n")

