from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.llms.ollama import Ollama
from llama_index.core.embeddings import resolve_embed_model
from llama_index.core import Settings

print("ğŸš€ å…è²» AI Agent å•Ÿå‹•ä¸­...\n")

# ============ è¨­å®šå…è²» LLM ============
print("ğŸ¤– é€£æ¥æœ¬åœ° Ollama...")
Settings.llm = Ollama(model="llama3.2:3b", request_timeout=120.0)
Settings.embed_model = resolve_embed_model("local:BAAI/bge-small-en-v1.5")
print("âœ… æœ¬åœ°æ¨¡å‹å·²è¼‰å…¥\n")

# ============ è¼‰å…¥ PDF ============
print("ğŸ“„ è¼‰å…¥ PDF...")
documents = SimpleDirectoryReader(
    input_files=["sample.pdf"]
).load_data()
print(f"âœ… å·²è¼‰å…¥ {len(documents)} å€‹æ–‡ä»¶\n")

# ============ å»ºç«‹ç´¢å¼• ============
print("ğŸ” å»ºç«‹å‘é‡ç´¢å¼•...")
index = VectorStoreIndex.from_documents(documents)
print("âœ… ç´¢å¼•å»ºç«‹å®Œæˆ\n")

# ============ æŸ¥è©¢å¼•æ“ ============
query_engine = index.as_query_engine()

# ============ æ¸¬è©¦ ============
print("="*60)
print("ğŸ¯ å…è²» AI Agent å·²æº–å‚™å¥½ï¼")
print("="*60 + "\n")

test_queries = [
    "é€™å€‹æ–‡ä»¶ä¸»è¦è¬›ä»€éº¼ï¼Ÿ",
    "æœ‰æåˆ°å“ªäº›æŠ€è¡“ï¼Ÿ"
]

for q in test_queries:
    print(f"ğŸ‘¤ Q: {q}")
    response = query_engine.query(q)
    print(f"ğŸ¤– A: {response}\n")

# ============ äº’å‹•æ¨¡å¼ ============
print("ğŸ’¬ é€²å…¥äº’å‹•æ¨¡å¼ (è¼¸å…¥ exit é€€å‡º)\n")
while True:
    user_input = input("ä½ : ").strip()
    if user_input.lower() in ['exit', 'quit']: break
    if user_input:
        response = query_engine.query(user_input)
        print(f"ğŸ¤– {response}\n")

